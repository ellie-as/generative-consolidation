{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling memory distortions with a variational autoencoder: the Carmichael experiment\n",
    "\n",
    "In an experiment by Carmichael et al. (1932), subjects were asked to reproduce ambiguous sketches. A context was established by telling the subjects that they would see images from a certain category. It was found that when the subjects tried to reproduce the image after a delay, their drawings were distorted to look more like members of the context class.\n",
    "\n",
    "Nagy et al. (2020) showed that a variational autoencoder trained on a class biases recall towards that class, but they used a separate model for each class (rather than a single model with context as an input). The data below extends this by using a single model with the context represented by a cue in the image.\n",
    "\n",
    "In this notebook, I train variational autoencoders on pairs of visually similar classes from various datasets. The results show that the model 'recalls' the same ambiguous image differently depending on the context; as in the Carmichael experiment, the 'recalled' image looks more like the context class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib.pyplot import imshow\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import random\n",
    "from utils import prepare_data, noise, display\n",
    "from initial_model import create_autoencoder\n",
    "from initial_tests import check_initial_recall, iterative_recall\n",
    "from generative_model import build_encoder_decoder_v1, VAE,build_encoder_decoder_v2, build_encoder_decoder_v3, build_encoder_decoder_v4, build_encoder_decoder_v5\n",
    "from generative_tests import interpolate_ims, plot_latent_space, check_generative_recall, plot_history, vector_arithmetic\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.datasets import mnist, fashion_mnist, cifar10\n",
    "import numpy as np\n",
    "from random import randrange\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import hopfield_utils\n",
    "from hopfield_models import ContinuousHopfield\n",
    "import matplotlib.backends.backend_pdf\n",
    "from config import models_dict, dims_dict\n",
    "\n",
    "tf.random.set_seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the data\n",
    "\n",
    "Based on Carmichael et al. (1932), let's pick a similar looking pairs of items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_data, train_label), (test_data, _) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class bar will be used to set context as data1\n",
    "class_1 = 2\n",
    "class_2 = 3\n",
    "\n",
    "data1 = [i for ind, i in enumerate(train_data) if train_label[ind] == class_2]\n",
    "data2 = [i for ind, i in enumerate(train_data) if train_label[ind] == class_1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add context cues\n",
    "\n",
    "Equivalent to the use of a word to set the context in Carmichael et al, I add added a feature to each image that indicates the class. This will allow us to manipulate the context and see how that effects the reconstruction. A horizontal line at the top of the image indicates a certain class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_class_bar(d):\n",
    "    d_bar = [255]*28 + list(d.flatten())[28:]\n",
    "    return np.array(d_bar).reshape((28,28,1))\n",
    "\n",
    "def preprocess(d):\n",
    "    d_bar = list(d.flatten())\n",
    "    return np.array(d_bar).reshape((28,28,1))\n",
    "\n",
    "def remove_class_bar(d):\n",
    "    d_bar = [0]*28 + list(d.flatten())[28:]\n",
    "    return np.array(d_bar).reshape((28,28,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 5000\n",
    "resized_train = [add_class_bar(d) for d in data1][0:num] + [preprocess(d) for d in data2][0:num] #+ [np.resize(add_class_bar(d), (28,28,1)) for d in data3][0:num] + [np.resize(d, (28,28,1)) for d in data4][0:num]\n",
    "train = np.stack(resized_train, axis=0).astype(\"float32\") / 255\n",
    "\n",
    "inverse_resized_train = [preprocess(d) for d in data1][0:num] + [add_class_bar(d) for d in data2][0:num] #+ [np.resize(add_class_bar(d), (28,28,1)) for d in data3][0:num] + [np.resize(d, (28,28,1)) for d in data4][0:num]\n",
    "inverse_train = np.stack(resized_train, axis=0).astype(\"float32\") / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot an item from the first class as an example to see how the context appears:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(resized_train[0].reshape((28,28)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(resized_train[num].reshape((28,28)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the VAE\n",
    "\n",
    "Using the functions defined above, build a VAE with two latent variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder, decoder = build_encoder_decoder_v5(latent_dim = 6)\n",
    "vae = VAE(encoder, decoder, kl_weighting=5)\n",
    "opt = keras.optimizers.Adam(lr=0.001)\n",
    "vae.compile(optimizer=opt)\n",
    "history = vae.fit(train, epochs=100, verbose=0, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore how context affects recall\n",
    "\n",
    "Let's see how the model affects the same drawing with and without the context cue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_a_with_cue = [np.resize(add_class_bar(d), (28,28,1)) for d in data1[0:10]]\n",
    "class_a_with_cue = np.array(class_a_with_cue).astype(\"float32\") / 255\n",
    "class_a_without_cue = [np.resize(d, (28,28,1)) for d in data1[0:10]]\n",
    "class_a_without_cue = np.array(class_a_without_cue).astype(\"float32\") / 255\n",
    "\n",
    "class_b_with_cue = [np.resize(add_class_bar(d), (28,28,1)) for d in data2[0:10]]\n",
    "class_b_with_cue = np.array(class_b_with_cue).astype(\"float32\") / 255\n",
    "class_b_without_cue = [np.resize(d, (28,28,1)) for d in data2[0:10]]\n",
    "class_b_without_cue = np.array(class_b_without_cue).astype(\"float32\") / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How does context affect recall of ambiguous items from the class_b category?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(5, 3, figsize=(6,8), sharex=True)\n",
    "\n",
    "for i in range(5):\n",
    "    item = class_b_without_cue[i+5]\n",
    "    axs[i,0].imshow(np.resize(item, (28,28)), cmap='Greys')\n",
    "    axs[i,0].axis('off')\n",
    "\n",
    "    encoding = encoder.predict(item.reshape(1,28,28,1))\n",
    "    x_decoded = decoder.predict(encoding)\n",
    "    axs[i,1].imshow(remove_class_bar(np.resize(x_decoded, (28,28))), cmap='Greys')\n",
    "    axs[i,1].axis('off')\n",
    "    \n",
    "    item = class_b_with_cue[i]\n",
    "    encoding = encoder.predict(item.reshape(1,28,28,1))\n",
    "    x_decoded = decoder.predict(encoding)\n",
    "    axs[i,2].imshow(remove_class_bar(np.resize(x_decoded, (28,28))), cmap='Greys')\n",
    "    axs[i,2].axis('off')\n",
    "    \n",
    "    \n",
    "for ax, col in zip(axs[0,:], ['Original', \n",
    "                              'Recalled (context 1)',  \n",
    "                              'Recalled (context 2)']):\n",
    "    ax.set_title(col, size=12)\n",
    "    \n",
    "fig.tight_layout() \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How does context affect recall of ambiguous items from the class_a category?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(5, 3, figsize=(6,8), sharex=True)\n",
    "\n",
    "for i in range(5):\n",
    "    item = class_a_without_cue[i+5]\n",
    "    axs[i,0].imshow(np.resize(item, (28,28)), cmap='Greys')\n",
    "    axs[i,0].axis('off')\n",
    "\n",
    "    encoding = encoder.predict(item.reshape(1,28,28,1))\n",
    "    x_decoded = decoder.predict(encoding)\n",
    "    axs[i,1].imshow(remove_class_bar(np.resize(x_decoded, (28,28))), cmap='Greys')\n",
    "    axs[i,1].axis('off')\n",
    "    \n",
    "    item = class_a_with_cue[i]\n",
    "    encoding = encoder.predict(item.reshape(1,28,28,1))\n",
    "    x_decoded = decoder.predict(encoding)\n",
    "    axs[i,2].imshow(remove_class_bar(np.resize(x_decoded, (28,28))), cmap='Greys')\n",
    "    axs[i,2].axis('off')\n",
    "    \n",
    "    \n",
    "for ax, col in zip(axs[0,:], ['Original', \n",
    "                              'Recalled (context 1)',  \n",
    "                              'Recalled (context 2)']):\n",
    "    ax.set_title(col, size=12)\n",
    "    \n",
    "fig.tight_layout() \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop through different KL weight / LD combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distortions(kl_weighting=1, l_d=4):\n",
    "    encoder, decoder = build_encoder_decoder_v5(latent_dim = l_d)\n",
    "    vae = VAE(encoder, decoder, kl_weighting=kl_weighting)\n",
    "    opt = keras.optimizers.Adam(lr=0.001)\n",
    "    vae.compile(optimizer=opt)\n",
    "    history = vae.fit(train, epochs=100, verbose=0, batch_size=32, shuffle=True)\n",
    "    \n",
    "    class_a_with_cue = [np.resize(add_class_bar(d), (28,28,1)) for d in data1[0:10]]\n",
    "    class_a_with_cue = np.array(class_a_with_cue).astype(\"float32\") / 255\n",
    "    class_a_without_cue = [np.resize(d, (28,28,1)) for d in data1[0:10]]\n",
    "    class_a_without_cue = np.array(class_a_without_cue).astype(\"float32\") / 255\n",
    "\n",
    "    class_b_with_cue = [np.resize(add_class_bar(d), (28,28,1)) for d in data2[0:10]]\n",
    "    class_b_with_cue = np.array(class_b_with_cue).astype(\"float32\") / 255\n",
    "    class_b_without_cue = [np.resize(d, (28,28,1)) for d in data2[0:10]]\n",
    "    class_b_without_cue = np.array(class_b_without_cue).astype(\"float32\") / 255\n",
    "    \n",
    "    fig, axs = plt.subplots(5, 3, figsize=(8,8), sharex=True)\n",
    "\n",
    "    for i in range(5):\n",
    "        item = class_b_without_cue[i]\n",
    "        axs[i,0].imshow(np.resize(item, (28,28)), cmap='Greys')\n",
    "        axs[i,0].axis('off')\n",
    "\n",
    "        encoding = encoder.predict(item.reshape(1,28,28,1))\n",
    "        x_decoded = decoder.predict(encoding)\n",
    "        axs[i,1].imshow(remove_class_bar(np.resize(x_decoded, (28,28))), cmap='Greys')\n",
    "        axs[i,1].axis('off')\n",
    "\n",
    "        item = class_b_with_cue[i]\n",
    "        encoding = encoder.predict(item.reshape(1,28,28,1))\n",
    "        x_decoded = decoder.predict(encoding)\n",
    "        axs[i,2].imshow(remove_class_bar(np.resize(x_decoded, (28,28))), cmap='Greys')\n",
    "        axs[i,2].axis('off')\n",
    "\n",
    "\n",
    "    for ax, col in zip(axs[0,:], ['Original:', \n",
    "                                  'Recalled (context={}):'.format(class_1),  \n",
    "                                  'Recalled (context={}):'.format(class_2)]):\n",
    "        ax.set_title(col, size=12)\n",
    "\n",
    "    fig.tight_layout() \n",
    "    #plt.show()\n",
    "    fig.savefig('./distortions/mnist_{}lv_{}kl_{}vs{}_1.png'.format(l_d, kl_weighting, class_1, class_2))\n",
    "    \n",
    "    fig, axs = plt.subplots(5, 3, figsize=(8,8), sharex=True)\n",
    "\n",
    "    for i in range(5):\n",
    "        item = class_a_without_cue[i]\n",
    "        axs[i,0].imshow(np.resize(item, (28,28)), cmap='Greys')\n",
    "        axs[i,0].axis('off')\n",
    "\n",
    "        encoding = encoder.predict(item.reshape(1,28,28,1))\n",
    "        x_decoded = decoder.predict(encoding)\n",
    "        axs[i,1].imshow(remove_class_bar(np.resize(x_decoded, (28,28))), cmap='Greys')\n",
    "        axs[i,1].axis('off')\n",
    "\n",
    "        item = class_a_with_cue[i]\n",
    "        encoding = encoder.predict(item.reshape(1,28,28,1))\n",
    "        x_decoded = decoder.predict(encoding)\n",
    "        axs[i,2].imshow(remove_class_bar(np.resize(x_decoded, (28,28))), cmap='Greys')\n",
    "        axs[i,2].axis('off')\n",
    "\n",
    "\n",
    "    for ax, col in zip(axs[0,:], ['Original:', \n",
    "                                  'Recalled (context={}):'.format(class_1),  \n",
    "                                  'Recalled (context={}):'.format(class_2)]):\n",
    "        ax.set_title(col, size=12)\n",
    "\n",
    "    fig.tight_layout() \n",
    "    #plt.show()\n",
    "    fig.savefig('./distortions/mnist_{}lv_{}kl_{}vs{}_2.png'.format(l_d, kl_weighting, class_1, class_2))\n",
    "    \n",
    "klws = [1]\n",
    "lds = [4]\n",
    "\n",
    "for ld in lds:\n",
    "    for klw in klws:\n",
    "        print(ld, klw)\n",
    "        distortions(kl_weighting=klw, l_d=ld)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other things of interest\n",
    "\n",
    "#### Effect of the number of latent variables\n",
    "\n",
    "Let's try building models with a different dimension latent space, and see how that effects these results. The following cells trains 4 models with 2, 4, 6, and 8 latent variables respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "models = []\n",
    "latent_dims = [2,4,6,8]\n",
    "\n",
    "for l_d in latent_dims:\n",
    "    print(\"Training model with {} latent variables.\".format(l_d))\n",
    "    encoder, decoder = build_encoder_decoder_v5(latent_dim=l_d)\n",
    "    vae = VAE(encoder, decoder)\n",
    "    vae.compile(optimizer=keras.optimizers.Adam(lr=0.001))\n",
    "    vae.fit(train, epochs=100, batch_size=128, verbose=False)\n",
    "    \n",
    "    models.append((encoder,decoder,vae))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the different models recall the same item.\n",
    "\n",
    "It appears that smaller the dimension of the latent space, the greater the distortion (this makes sense as the memory gets compressed more). In other words, a variational autoencoder model of memory suggests you have more gist-based distortion when the storage capacity of the 'semantic memory' is smaller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(10, 5, figsize=(8,12), sharex=True)\n",
    "\n",
    "for ind, (encoder, decoder, vae) in enumerate(models):\n",
    "    for i in range(10):\n",
    "        item = inverse_train[random.randrange(0,len(train))]\n",
    "        \n",
    "        axs[i,0].imshow(remove_class_bar(np.resize(item, (28,28))), cmap='Greys')\n",
    "        axs[i,0].axis('off')\n",
    "\n",
    "        encoding = encoder.predict(item.reshape(1,28,28,1))\n",
    "        x_decoded = decoder.predict(encoding)\n",
    "        axs[i,ind+1].imshow(remove_class_bar(np.resize(x_decoded, (28,28))), cmap='Greys')\n",
    "        axs[i,ind+1].axis('off')\n",
    "        \n",
    "for ax, col in zip(axs[0,:], ['Original', \n",
    "                              '2 L.V.', \n",
    "                              '4 L.V.', \n",
    "                              '6 L.V.',\n",
    "                              '8 L.V.']):\n",
    "    ax.set_title(col, size=9)\n",
    "        \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References\n",
    "\n",
    "Carmichael, L., Hogan, H. P., & Walter, A. A. (1932). An experimental study of the effect of language on the reproduction of visually perceived form. Journal of experimental Psychology, 15(1), 73.\n",
    "\n",
    "Nagy, D. G., Török, B., & Orbán, G. (2020). Optimal forgetting: Semantic compression of episodic memories. PLOS Computational Biology, 16(10), e1008367."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_tensorflow2_p36",
   "language": "python",
   "name": "conda_amazonei_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
