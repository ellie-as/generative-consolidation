{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consolidation simulations\n",
    "\n",
    "Work-in-progress code for modelling consolidation as teacher-student learning, in which initial representations of memories are replayed to train a generative model.\n",
    "\n",
    "To use this code, install the requirements and launch a jupyter notebook (or alternatively use AWS SageMaker, e.g. the conda_amazonei_tensorflow2_p36 kernel).\n",
    "\n",
    "#### End-to-end simulation example\n",
    "\n",
    "The following code snippet:\n",
    "* Trains a modern Hopfield network on the MNIST dataset of handwritten digits\n",
    "* Gives the Hopfield network random noise as an input, and gets the outputs (which should be memories)\n",
    "* Trains a variational autoencoder on the 'memories'\n",
    "* Runs a set of tests, e.g. tests recall and interpolation between items, and plots the latent space projected into 2D\n",
    "* Saves the outputs to a pdf in the 'outputs' folder (see example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from end_to_end import run_end_to_end\n",
    "import tensorflow as tf\n",
    "\n",
    "# set tensorflow random seed to make outputs reproducible\n",
    "tf.random.set_seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below recreates the results in the 'outputs' folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_end_to_end(initial='hopfield', generative='vae', dataset='shapes3d', generative_epochs=1000, \n",
    "               num=1000, latent_dim=10, kl_weighting=1)\n",
    "\n",
    "run_end_to_end(initial='hopfield', generative='vae', dataset='solids', generative_epochs=1000, \n",
    "               num=1000, latent_dim=10, kl_weighting=1)\n",
    "\n",
    "run_end_to_end(initial='hopfield', generative='vae', dataset='fashion_mnist', generative_epochs=1000, \n",
    "               num=1000, latent_dim=10, kl_weighting=1)\n",
    "\n",
    "run_end_to_end(initial='hopfield', generative='vae', dataset='mnist', generative_epochs=1000, \n",
    "               num=1000, latent_dim=10, kl_weighting=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The options can be swapped out to run different experiments, e.g. to try with an autoencoder as the initial model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_end_to_end(initial='autoencoder', generative='vae', dataset='mnist', initial_epochs=10, generative_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other examples\n",
    "\n",
    "Prepare the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data, noisy_train_data, noisy_test_data = prepare_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a modern Hopfield network and store 1000 MNIST memories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = hopfield_utils.create_hopfield(1000, hopfield_type='continuous')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, build and fit a denoising autoencoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = create_autoencoder()\n",
    "\n",
    "autoencoder.fit(\n",
    "    x=noisy_train_data,\n",
    "    y=train_data,\n",
    "    epochs=initial_epochs,\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    "    validation_data=(noisy_test_data, test_data),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display recall from random noise by the initial model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, fig = check_initial_recall(autoencoder, train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a variational autoencoder on replayed memories from the initial model (i.e. outputs when the initial model is presented with random noise), and plot the loss over time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder, decoder = build_encoder_decoder(latent_dim = 5)\n",
    "vae = VAE(encoder, decoder, kl_weighting=1)\n",
    "opt = keras.optimizers.Adam(lr=0.005, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=True)\n",
    "vae.compile(optimizer=opt)\n",
    "history = vae.fit(predictions, epochs=generative_epochs, verbose=0)\n",
    "\n",
    "fig = plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Hopfield network code is based on https://github.com/ml-jku/hopfield-layers\n",
    "The variational autoencoder code is based on https://github.com/keras-team/keras-io/blob/master/examples/generative/vae.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_tensorflow2_p36",
   "language": "python",
   "name": "conda_amazonei_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
